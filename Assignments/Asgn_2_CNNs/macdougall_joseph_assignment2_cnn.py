# -*- coding: utf-8 -*-
"""MacDougall_Joseph_Assignment2_CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NVDxa1jVZpmFy-HYqlFM1k18fAk3JnfV

### In the first section of the assignment, you will design a Convolutional Neural Network (CNN) to classify the Cifar-10 dataset. In the second section, you will design a Bean Disease Classifier to categorize healthy and diseased beans.

# 1.Create a CNN to classify Cifar-10

In the previous assignment you built a CNN that classified 10 different apparel from the Fashion MNIST dataset. For this section, you will use a similar network but there are some key differences you'll need to take into account. Learn about Cifar-10 here: https://www.cs.toronto.edu/~kriz/cifar.html

---


**Question 1**: Can you think of two differences in the model required to classify the Cifar-10 Dataset and the Fashion MNIST Dataset?

**Answer:** The model must account for color in Cifar-10 wheras Fashion MNIST is purely grayscale. Additionally, we can see that there is no class overlap/less similarity in the theme for Cifar-10 (MNIST is all centered around apparel). This, coupled with the fact that there is less standardization in image orientation, means that we will have to emphasize our convolutional layers in our Cifar-10 model.



---

### Load Dataset

We set up the problem by first loading the dataset and splitting into train and test sets.
"""

import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt


# load the dataset
(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

"""### Normalize Images"""

# Normalize pixel values to be between 0 and 1
train_images = train_images / 255.0
test_images = test_images / 255.0

"""### Visualize Sample Training Examples

Explore the images from the Cifar-10 dataset. Could it help you determine the input shape and the layers of the machine learning model?

First, print the shape of the training images, training labels, test images and test labels.
"""

# Print shapes of train and test sets
print("Training:", "Images Shape:", train_images.shape, "Labels Shape: ", train_labels.shape)
print("Testing:", "Images Shape: ",test_images.shape, "Labels Shape: ", test_labels.shape)

"""---


**Question 2:** Plot the first 3 training images along with the training labels.




---


"""

#Helper Fxn to Plot an Image and its Label
def show_training_image(img_index):
  plt.figure()
  plt.imshow(train_images[img_index])
  plt.title("Label: " + str(train_labels[img_index][0]))
  plt.grid(False)
  plt.show()

# Plot 1st training example
show_training_image(0)

# Plot 2nd training example
show_training_image(1)

# Plot 3rd training example
show_training_image(2)

"""### Define Model

It's now time to define your CNN. Experient with different no. of layers enough to get above 85% training accuracy and above 70% validation accuracy!

***Hint:*** your model may want to learn some high level features and then classify them i.e., first define Conv2D and MaxPooling2D layers followed by Dense layers.
"""

# Define your model layers!

model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])

model.summary()

"""Define an appropriate loss function and optimizer. Then compile your model!"""

# Define the loss function and optimizer and compile the model
OPT = 'adam'
LOSS = 'sparse_categorical_crossentropy'
model.compile(loss=LOSS,
              optimizer=OPT,
              metrics=['acc'])

"""### Train Model

Now train your model for a fixed number of epochs. You can experiment with this hyperparameter (NUM_EPOCHS) to get the desired accuracy (above 85% training accuracy and above 70% validation accuracy)
"""

NUM_EPOCHS = 8

# Fit the model
history = model.fit(train_images, train_labels, epochs=NUM_EPOCHS,
                    validation_data=(test_images, test_labels))

"""### Plot Accuracy vs Epochs

Finally, plot a single graph of both training and test (validation) accuracies vs Epochs. Be sure to include a legend and x & y axis labels.


***Hint1:*** Use information stored in 'history'.

***Hint2:*** Use plt.plot()
"""

# TODO: Plot train and test (validation) accuracies vs Epochs on a single plot
xaxis = range(NUM_EPOCHS)
plt.plot(xaxis, history.history['acc'], 'r',label="Training Accuracy")
plt.plot(xaxis, history.history['val_acc'], 'g', label="Test Accuracy")
plt.xlabel('Epochs')
plt.ylabel('Accuracies')
plt.legend(loc='upper left')
plt.title('Accuracies vs. Epochs')
plt.show()

"""# 2. Bean Disease Classifier
For this section of the assignment, you'll take what you've learned so far and build a classifier for bean diseases. You'll be provided with training and validation data of images taken of bean plants in Uganda. These images show healthy bean leaves as well as 2 types of common disease: bean rust and angular leaf spots. Your job will be to build a neural network that can tell the difference between the healthy and diseased leaves.

We start by setting up the problem for you.
"""

# Commented out IPython magic to ensure Python compatibility.
try:
  # %tensorflow_version only exists in Colab.
#   %tensorflow_version 2.x
except Exception:
  pass

import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt

"""### Load Dataset"""

!pip install --upgrade --no-cache-dir gdown

"""In the case, that the below cells do not work and the tmp/ directory refuses to show up. You can manually download the dataset from this [link](https://drive.google.com/drive/folders/1D5yto1n48ksf52iHLqwtiTy5_CWGUS1I?usp=share_link).

After that you can use the mount google drive option that we have commented below to use it directly. **You will need to modify the directory names for this to work.**
"""

# download train set
!gdown --id 1vunpRL_B5fxrLbro6t4GHDdqKKyZjPhm -O /tmp/train.zip

# download validation set
!gdown --id 1rGg-OB2V_V78-d02yjwh1xfwLcg9mMoI -O /tmp/validation.zip

# download test set
!gdown --id 1UOmf0T5w56DdXRWgmd4Gxvmz_21Xukpc -O /tmp/test.zip

"""The .zip files are extracted and loaded into three directories - train, validation, and test. Each of them further have three directories each representing the 3 classes.

You can find them in the **tmp/** directory in the files tab on the left.
"""

import os
import zipfile

local_zip = '/tmp/train.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/tmp')
local_zip = '/tmp/validation.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/tmp')
local_zip = '/tmp/test.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/tmp')
zip_ref.close()

"""### Display example images

Visualize some training examples from each of the three classes. Use the below defined function `show_img()` to plot an example image of the *angular_leaf_spot*, *bean_rust*, and *healthy* classes from the train directory.

The required data can be found in the tmp/ folder under Files on the left tab.

"""

import matplotlib.pyplot as plt
import matplotlib.image as mpimg

def show_img(path, file_name):
    image = mpimg.imread(path + file_name)
    plt.imshow(image)
    plt.show()

"""Plot an example from the ***angular_leaf_spot*** class.


***Hint:*** You can find the path of the required directory by right-clicking on the three vertical dots and choosing 'Copy path' (hover the cursor over the directory)
"""

# Define path and filename and display a training example
train_angular_leaf_spot_path = '/tmp/train/angular_leaf_spot/'
train_angular_leaf_spot_filename = 'angular_leaf_spot_train.0.jpg'

show_img(train_angular_leaf_spot_path,train_angular_leaf_spot_filename)

"""Plot an example from the ***bean_rust*** class."""

# Define path and filename and display a training example
train_bean_rust_path = '/tmp/train/bean_rust/'
train_bean_rust_filename = 'bean_rust_train.0.jpg'

show_img(train_bean_rust_path,train_bean_rust_filename)

"""Plot an example from the ***healthy*** class."""

# Define path and filename and display a training example
train_healthy_path = '/tmp/train/healthy/'
train_healthy_filename = 'healthy_train.0.jpg'

show_img(train_healthy_path,train_healthy_filename)

"""### Define Generator to include Image Augmentation

Define a generator (```ImageDataGenerator```) to process the train and validation data we have loaded in Colab so that our model can use it for training.

Include the following Image Augmentation parametes for the **train data generator**.

> rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest'.

Include the following Image Augmentation parametes for the **validation data generator**.

> rescale=1./255


"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# < YOUR CODE HERE >
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# < YOUR CODE HERE >
validation_datagen = ImageDataGenerator(
    rescale=1./255
)


TRAIN_DIRECTORY_LOCATION = '/tmp/train/'
VAL_DIRECTORY_LOCATION = '/tmp/validation/'
TARGET_SIZE = target_size=(224, 224)
CLASS_MODE = 'categorical'

train_generator = train_datagen.flow_from_directory(
    TRAIN_DIRECTORY_LOCATION,
    target_size = TARGET_SIZE,
    batch_size = 128,
    class_mode = CLASS_MODE
)

validation_generator = validation_datagen.flow_from_directory(
    VAL_DIRECTORY_LOCATION,
    target_size = TARGET_SIZE,
    batch_size = 128,
    class_mode = CLASS_MODE
)

"""### Define Model

Now its your turn to define a model to learn this data. Experient with different no. of layers enough to get above 75% training accuracy and above 70% validation accuracy! Like with the CIFAR-10 assignment, your model may want to learn some high level features and then classify them.

**Note:** Be careful of the input shape of the first Conv2D layer.


"""

# Define CNN
model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3)),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(3, activation=tf.nn.softmax)
])

"""Print your model summary!"""

# Print a summary of your model
model.summary()

"""---


**Question 3**: Why are the number of parameters learned in the MaxPooling layers =  0?

**Answer:** Pooling layers don't learn: they don't have any weights or biases. They will execute the same for every iteration of the model by taking a region of the image and outputting the max value.

---

Pick an appropriate loss function and optimizer. Then compile your model! Don't forget to define your metric!
"""

# Define Loss Function and Optimizer. Compile Model
OPT = 'adam'
LOSS = 'categorical_crossentropy'
model.compile(loss=LOSS,
              optimizer=OPT,
              metrics=['acc'])

"""### Train Model

Now train your model for a fixed number of epochs. You can experiment with this hyperparameter (NUM_EPOCHS) to get the desired accuracy (above 75% training accuracy and above 70% validation accuracy)

"""

NUM_EPOCHS = 30


history = model.fit(
      train_generator,
      epochs = NUM_EPOCHS,
      verbose = 1,
      validation_data = validation_generator)

"""### Plot Accuracy vs Epochs

Finally, similar to section 1, plot a graph of both training and test (validation) accuracies vs Epochs. Include a legend and x & y axis labels.

"""

# Plot train and test accuracies vs Epochs
xaxis = range(NUM_EPOCHS)
plt.plot(xaxis, history.history['acc'], 'r',label="Training Accuracy")
plt.plot(xaxis, history.history['val_acc'], 'g', label="Test Accuracy")
plt.xlabel('Epochs')
plt.ylabel('Accuracies')
plt.legend(loc='upper left')
plt.title('Accuracies vs. Epochs')
plt.show()

